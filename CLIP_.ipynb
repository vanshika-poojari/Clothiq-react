{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6EgSpl+STYIfpbpqoq7GB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanshika-poojari/Clothiq-react/blob/main/CLIP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UvlJeaWrvZ4",
        "outputId": "0acfcf23-c7da-46d2-8a7c-344a6fb285d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "\n",
            "===== Single Word Similarity Results =====\n",
            "Image 0: BEST WORD = \"sunset\" (sim=0.2658) | WORST WORD = \"toaster\" (sim=0.1769)\n",
            "Image 1: BEST WORD = \"dog\" (sim=0.2825) | WORST WORD = \"sunset\" (sim=0.1690)\n",
            "Image 2: BEST WORD = \"forest\" (sim=0.2677) | WORST WORD = \"flower\" (sim=0.1584)\n",
            "Image 3: BEST WORD = \"dog\" (sim=0.2535) | WORST WORD = \"sunset\" (sim=0.1543)\n",
            "Image 4: BEST WORD = \"room\" (sim=0.2802) | WORST WORD = \"tree\" (sim=0.1699)\n",
            "\n",
            "===== Structured Caption Similarity Results =====\n",
            "Image 0: BEST STRUCTURED = \"A photo of a sunset\" (sim=0.2787) | WORST STRUCTURED = \"A photo of a building\" (sim=0.1916)\n",
            "Image 1: BEST STRUCTURED = \"A photo of a dog\" (sim=0.2922) | WORST STRUCTURED = \"A photo of a forest\" (sim=0.1741)\n",
            "Image 2: BEST STRUCTURED = \"A photo of a woods\" (sim=0.2921) | WORST STRUCTURED = \"A photo of a flower\" (sim=0.1763)\n",
            "Image 3: BEST STRUCTURED = \"A photo of a dog\" (sim=0.2593) | WORST STRUCTURED = \"A photo of a sunset\" (sim=0.1331)\n",
            "Image 4: BEST STRUCTURED = \"A photo of a bed\" (sim=0.2846) | WORST STRUCTURED = \"A photo of a tree\" (sim=0.1674)\n",
            "\n",
            "===== Arbitrary Caption Results =====\n",
            "Image 0: LOWEST SIM CAPTION = \"Aerial view of a city at night with neon signs.\" (sim=0.0824) | HIGHEST SIM CAPTION = \"An astronaut riding a horse on the moon.\" (sim=0.1468)\n",
            "Image 1: LOWEST SIM CAPTION = \"A plate of sushi served on a wooden board.\" (sim=0.0892) | HIGHEST SIM CAPTION = \"A cartoon illustration of a running dog with floppy ears.\" (sim=0.2333)\n",
            "Image 2: LOWEST SIM CAPTION = \"Aerial view of a city at night with neon signs.\" (sim=0.0753) | HIGHEST SIM CAPTION = \"A cartoon illustration of a running dog with floppy ears.\" (sim=0.1869)\n",
            "Image 3: LOWEST SIM CAPTION = \"Aerial view of a city at night with neon signs.\" (sim=0.1010) | HIGHEST SIM CAPTION = \"A cartoon illustration of a running dog with floppy ears.\" (sim=0.3615)\n",
            "Image 4: LOWEST SIM CAPTION = \"An astronaut riding a horse on the moon.\" (sim=0.1199) | HIGHEST SIM CAPTION = \"A messy hotel room with an iron board in the middle.\" (sim=0.2977)\n",
            "\n",
            "For Part 2: You can extend this code by providing your own captions dataset (like COCO Captions or LAION tags) and comparing similarities to find the overall highest scoring image-caption pair.\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# CLIP Cosine Similarity Experiment (All 5 Images)\n",
        "# ===============================================\n",
        "\n",
        "# 1) Install dependencies\n",
        "!pip install transformers ftfy regex tqdm torch torchvision pillow\n",
        "\n",
        "# 2) Imports\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 3) Load model & processor\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "\n",
        "# 4) Helper functions\n",
        "def image_from_url(url):\n",
        "    resp = requests.get(url, stream=True)\n",
        "    resp.raise_for_status()\n",
        "    return Image.open(resp.raw).convert('RGB')\n",
        "\n",
        "def embed_image(image):\n",
        "    inputs = processor(images=image, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        img_emb = model.get_image_features(**inputs)\n",
        "    img_emb = F.normalize(img_emb, dim=-1)\n",
        "    return img_emb.cpu()\n",
        "\n",
        "def embed_text(texts):\n",
        "    inputs = processor(text=texts, return_tensors='pt', padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        txt_emb = model.get_text_features(**inputs)\n",
        "    txt_emb = F.normalize(txt_emb, dim=-1)\n",
        "    return txt_emb.cpu()\n",
        "\n",
        "# 5) All Images (3 previous + 2 Flickr images)\n",
        "image_urls = [\n",
        "    # Replace these with your original 3 images if needed\n",
        "    'https://images.pexels.com/photos/36744/agriculture-arable-clouds-countryside.jpg',\n",
        "    'https://images.pexels.com/photos/825947/pexels-photo-825947.jpeg',\n",
        "    'https://images.pexels.com/photos/34044163/pexels-photo-34044163.jpeg',\n",
        "    # New Flickr images\n",
        "    'https://live.staticflickr.com/840/43380549381_004601c7ac_h.jpg',\n",
        "    'https://live.staticflickr.com/2404/2020522557_d1aa0a1066_k.jpg'\n",
        "]\n",
        "\n",
        "# 6) Download all images\n",
        "images = []\n",
        "for url in image_urls:\n",
        "    try:\n",
        "        img = image_from_url(url)\n",
        "        images.append(img)\n",
        "    except Exception as e:\n",
        "        print('Failed to download', url, e)\n",
        "        images.append(None)\n",
        "\n",
        "# 7) Precompute image embeddings\n",
        "img_embs = []\n",
        "for img in images:\n",
        "    if img is None:\n",
        "        img_embs.append(None)\n",
        "    else:\n",
        "        img_embs.append(embed_image(img))\n",
        "\n",
        "# 8) Define word list\n",
        "words = [\n",
        "    'sunset', 'sun', 'tree', 'bird', 'dog', 'cat', 'forest', 'woods',\n",
        "    'car', 'building', 'city', 'beach', 'mountain', 'person', 'man',\n",
        "    'woman', 'child', 'flower', 'food', 'toaster', 'room', 'bed', 'hotel', 'iron'\n",
        "]\n",
        "\n",
        "# 9) Compute best and worst single-word matches\n",
        "print(\"\\n===== Single Word Similarity Results =====\")\n",
        "for i, emb in enumerate(img_embs):\n",
        "    if emb is None:\n",
        "        print(f'Image {i}: no embedding')\n",
        "        continue\n",
        "    text_embs = embed_text(words)\n",
        "    sims = (emb @ text_embs.T).squeeze(0)\n",
        "    best_idx = sims.argmax().item()\n",
        "    worst_idx = sims.argmin().item()\n",
        "    print(f'Image {i}: BEST WORD = \"{words[best_idx]}\" (sim={sims[best_idx].item():.4f}) | WORST WORD = \"{words[worst_idx]}\" (sim={sims[worst_idx].item():.4f})')\n",
        "\n",
        "# 10) Structured captions (\"A photo of a W\")\n",
        "print(\"\\n===== Structured Caption Similarity Results =====\")\n",
        "structured = [f'A photo of a {w}' for w in words]\n",
        "for i, emb in enumerate(img_embs):\n",
        "    if emb is None:\n",
        "        continue\n",
        "    st_embs = embed_text(structured)\n",
        "    sims = (emb @ st_embs.T).squeeze(0)\n",
        "    best_idx = sims.argmax().item()\n",
        "    worst_idx = sims.argmin().item()\n",
        "    print(f'Image {i}: BEST STRUCTURED = \"{structured[best_idx]}\" (sim={sims[best_idx].item():.4f}) | WORST STRUCTURED = \"{structured[worst_idx]}\" (sim={sims[worst_idx].item():.4f})')\n",
        "\n",
        "# 11) Arbitrary captions (example sentences)\n",
        "print(\"\\n===== Arbitrary Caption Results =====\")\n",
        "candidates = [\n",
        "    'A close-up of a smiling baby wearing a party hat.',\n",
        "    'A red sports car driving at high speed on a racetrack.',\n",
        "    'Aerial view of a city at night with neon signs.',\n",
        "    'A plate of sushi served on a wooden board.',\n",
        "    'An astronaut riding a horse on the moon.',\n",
        "    'A messy hotel room with an iron board in the middle.',\n",
        "    'A cartoon illustration of a running dog with floppy ears.'\n",
        "]\n",
        "for i, emb in enumerate(img_embs):\n",
        "    if emb is None:\n",
        "        continue\n",
        "    cand_embs = embed_text(candidates)\n",
        "    sims = (emb @ cand_embs.T).squeeze(0)\n",
        "    best_idx = sims.argmax().item()\n",
        "    worst_idx = sims.argmin().item()\n",
        "    print(f'Image {i}: LOWEST SIM CAPTION = \"{candidates[worst_idx]}\" (sim={sims[worst_idx].item():.4f}) | HIGHEST SIM CAPTION = \"{candidates[best_idx]}\" (sim={sims[best_idx].item():.4f})')\n",
        "\n",
        "# 12) Notes for Part 2\n",
        "print(\"\\nFor Part 2: You can extend this code by providing your own captions dataset (like COCO Captions or LAION tags) and comparing similarities to find the overall highest scoring image-caption pair.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === PART 2: Find (image, caption) pair with largest cosine similarity ===\n",
        "\n",
        "import itertools\n",
        "\n",
        "# Some high-quality, regular real-world images\n",
        "image_urls_part2 = [\n",
        "    # 1. Red apple\n",
        "    \"https://images.pexels.com/photos/39803/pexels-photo-39803.jpeg\",\n",
        "\n",
        "    # 2. White cat\n",
        "    \"https://images.pexels.com/photos/127028/pexels-photo-127028.jpeg\",\n",
        "\n",
        "    # 3. Pepperoni pizza\n",
        "    \"https://images.pexels.com/photos/315755/pexels-photo-315755.jpeg\",\n",
        "\n",
        "    # 4. Mountain landscape\n",
        "    \"https://images.pexels.com/photos/674010/pexels-photo-674010.jpeg\",\n",
        "\n",
        "    # 5. Person with laptop (working replacement)\n",
        "    \"https://images.pexels.com/photos/4065876/pexels-photo-4065876.jpeg\"\n",
        "]\n",
        "\n",
        "# Candidate captions (normal, descriptive, not artificial)\n",
        "captions = [\n",
        "    \"A photo of a red apple on a white background.\",\n",
        "    \"A close-up photo of a white cat with blue eyes.\",\n",
        "    \"A delicious pepperoni pizza on a wooden table.\",\n",
        "    \"A scenic photo of mountains under a blue sky.\",\n",
        "    \"A person typing on a laptop at a desk.\"\n",
        "]\n",
        "\n",
        "# Compute embeddings\n",
        "imgs = [image_from_url(url) for url in image_urls_part2]\n",
        "img_embs2 = [embed_image(img) for img in imgs]\n",
        "text_embs2 = embed_text(captions)\n",
        "\n",
        "# Compute cosine similarities between every image & every caption\n",
        "sims = []\n",
        "for i, emb_i in enumerate(img_embs2):\n",
        "    for j, emb_t in enumerate(text_embs2):\n",
        "        score = float((emb_i @ emb_t.T).item())\n",
        "        sims.append((score, i, j))\n",
        "\n",
        "# Sort and print best matches\n",
        "sims.sort(reverse=True)\n",
        "best_score, best_img, best_cap = sims[0]\n",
        "\n",
        "print(f\"Highest similarity: {best_score:.4f}\")\n",
        "print(f\"Image URL: {image_urls_part2[best_img]}\")\n",
        "print(f\"Caption: {captions[best_cap]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BB1XFPhzUIc",
        "outputId": "fcd05f18-d22e-46c5-9901-3d9cfd066a09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest similarity: 0.3338\n",
            "Image URL: https://images.pexels.com/photos/39803/pexels-photo-39803.jpeg\n",
            "Caption: A photo of a red apple on a white background.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3202317276.py:41: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4421.)\n",
            "  score = float((emb_i @ emb_t.T).item())\n"
          ]
        }
      ]
    }
  ]
}